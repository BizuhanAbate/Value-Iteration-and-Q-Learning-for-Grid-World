{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "XX1zYyvtTUu2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define grid world parameters\n",
        "GRID_SIZE = 5\n",
        "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
        "GAMMA = 0.9  # Discount factor\n",
        "ALPHA = 0.5  # Learning rate\n",
        "EPISODES = 500  # Number of episodes\n",
        "REWARD_STEP = -1\n",
        "REWARD_GOAL = 10\n",
        "GOAL_STATE = (4, 4)"
      ],
      "metadata": {
        "id": "b7szMZXJTWLq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_state(state, action):\n",
        "    i, j = state\n",
        "    ni, nj = i + action[0], j + action[1]\n",
        "    if 0 <= ni < GRID_SIZE and 0 <= nj < GRID_SIZE:\n",
        "        return (ni, nj)\n",
        "    return state  # If hitting wall, stay in place\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))"
      ],
      "metadata": {
        "id": "MSYyhR4yTYYt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LKTL5zD-S-2a"
      },
      "outputs": [],
      "source": [
        "def q_learning():\n",
        "    for _ in range(EPISODES):\n",
        "        state = (0, 0)  # Start state\n",
        "        while state != GOAL_STATE:\n",
        "            action_idx = random.choice(range(len(ACTIONS)))  # Random action selection\n",
        "            action = ACTIONS[action_idx]\n",
        "            next_state = get_next_state(state, action)\n",
        "            reward = REWARD_GOAL if next_state == GOAL_STATE else REWARD_STEP\n",
        "\n",
        "            # Q-learning update rule\n",
        "            best_next_q = np.max(Q[next_state[0], next_state[1]])\n",
        "            Q[state[0], state[1], action_idx] += ALPHA * (reward + GAMMA * best_next_q - Q[state[0], state[1], action_idx])\n",
        "\n",
        "            state = next_state  # Move to the next state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Q-learning\n",
        "q_learning()\n",
        "\n",
        "# Print learned Q-values\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        print(f\"State ({i}, {j}): {Q[i, j]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RiQVfvITc5F",
        "outputId": "3190c5f5-2d43-472e-b6d1-a304db89929d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State (0, 0): [-1.3906558 -0.434062  -1.3906558 -0.434062 ]\n",
            "State (0, 1): [-0.434062   0.62882   -1.3906558  0.62882  ]\n",
            "State (0, 2): [ 0.62882   1.8098   -0.434062  1.8098  ]\n",
            "State (0, 3): [1.8098  3.122   0.62882 3.122  ]\n",
            "State (0, 4): [3.122  4.58   1.8098 3.122 ]\n",
            "State (1, 0): [-1.3906558  0.62882   -0.434062   0.62882  ]\n",
            "State (1, 1): [-0.434062  1.8098   -0.434062  1.8098  ]\n",
            "State (1, 2): [0.62882 3.122   0.62882 3.122  ]\n",
            "State (1, 3): [1.8098 4.58   1.8098 4.58  ]\n",
            "State (1, 4): [3.122 6.2   3.122 4.58 ]\n",
            "State (2, 0): [-0.434062  1.8098    0.62882   1.8098  ]\n",
            "State (2, 1): [0.62882 3.122   0.62882 3.122  ]\n",
            "State (2, 2): [1.8098 4.58   1.8098 4.58  ]\n",
            "State (2, 3): [3.122 6.2   3.122 6.2  ]\n",
            "State (2, 4): [4.58 8.   4.58 6.2 ]\n",
            "State (3, 0): [0.62882 3.122   1.8098  3.122  ]\n",
            "State (3, 1): [1.8098 4.58   1.8098 4.58  ]\n",
            "State (3, 2): [3.122 6.2   3.122 6.2  ]\n",
            "State (3, 3): [4.58 8.   4.58 8.  ]\n",
            "State (3, 4): [ 6.2 10.   6.2  8. ]\n",
            "State (4, 0): [1.8098 3.122  3.122  4.58  ]\n",
            "State (4, 1): [3.122 4.58  3.122 6.2  ]\n",
            "State (4, 2): [4.58 6.2  4.58 8.  ]\n",
            "State (4, 3): [ 6.2  8.   6.2 10. ]\n",
            "State (4, 4): [0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}